{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a666694-dcb4-4cf1-ad20-e92582637499",
   "metadata": {},
   "source": [
    "# Build a reverse image search engine with Amazon Rekognition, Amazon OpenSearch Service, and Amazon Titan Multimodal Embeddings in Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78ea54-da35-43f8-ae52-c42fb8315acd",
   "metadata": {},
   "source": [
    "This guide outlines how to build a reverse image search engine using the Amazon Titan Multimodal Embedding Model. It details the steps to embed images as vectors using Amazon Bedrock, store these embeddings in an Amazon OpenSearch Service Serverless vector index, and use Amazon Rekognition to extract key objects from images for querying the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e30c94-c492-4592-9bed-f9e395ad92d2",
   "metadata": {},
   "source": [
    "## Step 1: Install necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e530b-db3f-4665-96c7-ee1e4ff4e419",
   "metadata": {},
   "source": [
    "To retrieve images from our S3 bucket, utilize the Titan Multimodal Embeddings Model, and ingest embeddings into the Amazon OpenSearch index, we will need to install various Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40143308-a698-4951-967c-5499ffd17b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 pandas opensearch-py requests-aws4auth Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a25026-6fe0-40bb-913a-e5ec7a65c5d0",
   "metadata": {},
   "source": [
    "## Step 2: Create vector embeddings of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81aeea-798c-478a-9508-cf09ba281978",
   "metadata": {},
   "source": [
    "Once the images are loaded into the S3 bucket, we can then create embeddings for each image file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51e986-ee55-47bd-961c-aa02d2c658fb",
   "metadata": {},
   "source": [
    "### 2.1 Create function to handle embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587264c-7b20-46cb-b020-ff64c09e4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# Constants, change to your S3 bucket name and selected AWS region\n",
    "BUCKET_NAME = \"<YOUR_S3_BUCKET_NAME>\"\n",
    "BEDROCK_MODEL_ID = \"amazon.titan-embed-image-v1\"\n",
    "REGION = \"<YOUR_AWS_REGION>\" # For example, us-east-1\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3 = boto3.client('s3')\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    REGION, \n",
    "    endpoint_url=f\"https://bedrock-runtime.{REGION}.amazonaws.com\"\n",
    ")\n",
    "\n",
    "# Function to create embedding from input image\n",
    "def create_image_embedding(image):\n",
    "    image_input = {}\n",
    "\n",
    "    if image is not None:\n",
    "        image_input[\"inputImage\"] = image\n",
    "    else:\n",
    "        raise ValueError(\"Image input is required\")\n",
    "\n",
    "    image_body = json.dumps(image_input)\n",
    "\n",
    "    # Invoke Amazon Bedrock with encoded image body\n",
    "    bedrock_response = bedrock_client.invoke_model(\n",
    "        body=image_body,\n",
    "        modelId=BEDROCK_MODEL_ID,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    # Retrieve body in JSON response\n",
    "    final_response = json.loads(bedrock_response.get(\"body\").read())\n",
    "\n",
    "    embedding_error = final_response.get(\"message\")\n",
    "\n",
    "    if embedding_error is not None:\n",
    "        print (f\"Error creating embeddings: {embedding_error}\")\n",
    "\n",
    "    # Return embedding value\n",
    "    return final_response.get(\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41465d3f-e5d0-4a92-9e47-4dbdc1cfacb3",
   "metadata": {},
   "source": [
    "### 2.2 Loop through S3 bucket and embed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588efba-3f5f-4288-af93-eca562d597ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve images stored in S3 bucket \n",
    "response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "contents = response.get('Contents', [])\n",
    "\n",
    "# Define arrays to hold embeddings and image file key names\n",
    "image_embeddings = []\n",
    "image_file_names= []\n",
    "\n",
    "# Loop through S3 bucket to encode each image, generate it's embedding, and append to array\n",
    "for obj in contents:\n",
    "    image_data = s3.get_object(Bucket=BUCKET_NAME, Key=obj['Key'])['Body'].read()\n",
    "    base64_encoded_image = base64.b64encode(image_data).decode('utf-8') # Create base64 encoded image for Titan Multimodal Embeddings model input\n",
    "    image_embedding = create_image_embedding(image=base64_encoded_image)\n",
    "    image_embeddings.append(image_embedding)\n",
    "    image_file_names.append(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5d9ac-faa2-4e7d-adc8-fc70982fd8ec",
   "metadata": {},
   "source": [
    "### 2.3 Create dataframe to store embeddings and key of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1b911-7cad-4f08-9e30-1a08f8edf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and list embeddings with associated image file key to dataframe object\n",
    "final_embeddings_dataset = pd.DataFrame({'image_key': image_file_names, 'image_embedding': image_embeddings})\n",
    "final_embeddings_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d35605-5dea-4486-9af2-bcf9734b96f8",
   "metadata": {},
   "source": [
    "## 3. Ingest embeddings into OpenSearch Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d0f47-8fe5-446c-ba73-346666133871",
   "metadata": {},
   "source": [
    "With all the embeddings now generated, we can send them to the OpenSearch Serverless vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2739652-6ad3-4ede-93ac-198ea14ef5d2",
   "metadata": {},
   "source": [
    "### 3.1 Get SageMaker role to update IAM policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c02729-3428-4319-920a-b9dc964fb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, print SageMaker execution role to update IAM policy for read/write permissions to OpenSearch collection\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145ba02-56e7-41c8-8d31-13e3f5be6eb8",
   "metadata": {},
   "source": [
    "### 3.2 Connect to OpenSearch Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1a584-bd0a-4a6e-a02c-648cd6c583b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries to connect to Amazon OpenSearch Serverless connection\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "# Initialize endpoint name constant\n",
    "HOST = \"<YOUR_HOST_ENDPOINT_NAME\"> # For example, abcdefghi.us-east-1.aoss.amazonaws.com (without https://)\n",
    "\n",
    "# Initialize and authenticate with the OpenSearch client\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWS4Auth(credentials.access_key, credentials.secret_key, REGION, 'aoss', session_token=credentials.token)\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': HOST, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=300\n",
    ")\n",
    "\n",
    "# Print client connection\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e6602-256f-4901-9b8d-d2f93947fa11",
   "metadata": {},
   "source": [
    "### 3.3 Ingest embeddings and key to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa7192-f443-4383-972a-5aaf352f138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library to iterate through dataset\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "# Your vector index and mapping fields \n",
    "INDEX_NAME = \"<YOUR_VECTOR_INDEX_NAME>\"\n",
    "VECTOR_NAME = \"<YOUR_VECTOR_FIELD_NAME>\"\n",
    "VECTOR_MAPPING = \"<YOUR_MAPPING_FIELD_NAME>\"\n",
    "\n",
    "# Ingest embeddings into vector index with associate vector and text mapping fields\n",
    "for idx, record in tq.tqdm(final_embeddings_dataset.iterrows(), total=len(final_embeddings_dataset)):\n",
    "    body = {\n",
    "        VECTOR_NAME: record['image_embedding'],\n",
    "        VECTOR_MAPPING: record['image_key']\n",
    "    }\n",
    "    response = client.index(index=INDEX_NAME, body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917256d-d900-4e02-b11b-03f5c7c77e7e",
   "metadata": {},
   "source": [
    "## 4. Extract key objects from images with Amazon Rekognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa041695-f115-4dc9-a4e1-f4adbf57ab06",
   "metadata": {},
   "source": [
    "To extract a particular object of interest in an image (for example, a shoe), we can use Amazon Rekognition to detect the label and save the extracted object as a new image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44e64b-c1f9-475d-9986-ac96ac2a5c1f",
   "metadata": {},
   "source": [
    "### 4.1 Detect labels in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf5124-992d-4dd6-a547-36469ee3e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Amazon Rekognition client\n",
    "rekognition = boto3.client('rekognition')\n",
    "\n",
    "# Function to detect and return all labels in a given image\n",
    "def detect_objects(file_name):\n",
    "    \n",
    "    with open(file_name, 'rb') as image:\n",
    "        response = rekognition.detect_labels(Image={'Bytes': image.read()})\n",
    "\n",
    "    # Define arrays to hold box coordinates and label titles with confidence scores\n",
    "    boxes = []\n",
    "    labels = []\n",
    "\n",
    "    # Print each label detected and the associated confidence score\n",
    "    for label in response['Labels']:\n",
    "        print(\"Label: \" + label['Name'])\n",
    "        print(\"Confidence: \" + str(label['Confidence']) + \"%\\n\")\n",
    "        if 'Instances' in label:\n",
    "            for instance in label['Instances']:\n",
    "                boxes.append(instance['BoundingBox'])\n",
    "                labels.append(label['Name'])\n",
    "\n",
    "    return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad500dbe-f433-4ac2-878d-a894ecd37ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect labels and bounding boxes in test image\n",
    "INPUT_SEARCH_IMAGE = '<YOUR_TEST_IMAGE_FILE (ex. test-image.jpeg)>'  # Update with the path to your image file in the local SageMaker environment\n",
    "boxes, labels = detect_objects(INPUT_SEARCH_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d607d-97c6-45a4-8e13-0251f6d82bfd",
   "metadata": {},
   "source": [
    "### 4.2 Draw bounding boxes for labelled objects of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9bab6-bd3d-490a-b03d-d48afbe89017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries to draw bounding box on image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Function to take in bounding box coordinates and draw box around label of interest\n",
    "def draw_bounding_box(image_path, boxes, labels):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert RGBA to RGB if necessary\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Font for the label\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Draw bounding boxes around specific label of interest (ex. shoe)\n",
    "    for box, label in zip(boxes, labels):\n",
    "        if label not in \"Shoe\":\n",
    "            continue\n",
    "        else:\n",
    "            # Box coordinates\n",
    "            left = image.width * box['Left']\n",
    "            top = image.height * box['Top']\n",
    "            width = image.width * box['Width']\n",
    "            height = image.height * box['Height']\n",
    "    \n",
    "        # Draw rectangle\n",
    "        draw.rectangle([left, top, left + width, top + height], outline=\"red\")\n",
    "    \n",
    "        # Draw label\n",
    "        draw.text((left, top), label, fill=\"red\", font=font)\n",
    "        \n",
    "    # Save or display the image\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8912c-65fe-4676-ad51-ac7e48090b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bounding_box(INPUT_SEARCH_IMAGE, boxes, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a017d3-19ee-41b9-845b-403428a2b6c2",
   "metadata": {},
   "source": [
    "### 4.3 Extract and save labelled objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7de0a-5448-4c7b-87c6-782630e70d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code as above, only now to save each labeled object as a separate image\n",
    "\n",
    "# Import required libraries to draw bounding box on image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Function to extract and save each labelled object as a separate image\n",
    "def extract_objects(image_path, boxes, labels):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert RGBA to RGB if necessary\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Font for the label\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Counter for unique filenames\n",
    "    crop_count = 1 \n",
    "    \n",
    "    # Draw bounding boxes around specific label of interest (ex. shoe)\n",
    "    for box, label in zip(boxes, labels):\n",
    "        if label not in \"Shoe\":\n",
    "            continue\n",
    "    \n",
    "        # Box coordinates\n",
    "        left = int(image.width * box['Left'])\n",
    "        top = int(image.height * box['Top'])\n",
    "        right = left + int(image.width * box['Width'])\n",
    "        bottom = top + int(image.height * box['Height'])\n",
    "    \n",
    "        # Crop the image to the bounding box\n",
    "        cropped_image = image.crop((left, top, right, bottom))\n",
    "    \n",
    "        # Draw label on the cropped image\n",
    "        draw = ImageDraw.Draw(cropped_image)\n",
    "    \n",
    "        # File name for the output\n",
    "        file_name = f\"extract_{crop_count}.jpg\"\n",
    "        # Save extracted object image locally\n",
    "        cropped_image.save(file_name)\n",
    "        print(f\"Saved extracted object image: {file_name}\")\n",
    "        crop_count += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac4f8c-adb9-4d66-86d7-585363910fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_objects(INPUT_SEARCH_IMAGE, boxes, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba640b85-6e67-46c0-970c-8d4481de9eaf",
   "metadata": {},
   "source": [
    "## 5. Embed extracted object image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f75a6-a083-40ad-a239-91266c0a7937",
   "metadata": {},
   "source": [
    "Now that we have extracted the object of interest from the input image, we can embed the image in a vector representation so it can be queried against the OpenSearch collection index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a89160-457e-4db3-bd2c-34339a1d7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the extracted object image file in binary mode\n",
    "# The extracted images will be saved as extract_1.jpg, extract_2.jpg, etc.\n",
    "with open(\"<YOUR_LOCAL_EXTRACTED_IMAGE (ex. extract_1.jpg)>\", \"rb\") as image_file:\n",
    "    base64_encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Embed the extracted object image\n",
    "object_embedding = create_image_embedding(image=base64_encoded_image)\n",
    "\n",
    "# Print the first few numbers of the embedding followed by ...\n",
    "print(f\"Image embedding: {object_embedding[:5]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992576e-7ca4-48ff-95d8-c7b6ea7efcc7",
   "metadata": {},
   "source": [
    "## 6. Perform reverse image search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86193eb2-9e99-4f7c-8f0a-a99db31fbad1",
   "metadata": {},
   "source": [
    "With the embedded extracted object image, we can now query against the vector index using a K-NN search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7624d-18ee-4382-9090-dfa9cd14b567",
   "metadata": {},
   "source": [
    "### 6.1 Perform a K-NN search on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac96e9-82d7-47af-bc75-e36e6a343d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize endpoint name constant\n",
    "HOST = \"<YOUR_HOST_ENDPOINT_NAME\"> # For example, abcdefghi.us-east-1.aoss.amazonaws.com (without https://)\n",
    "\n",
    "# Initialize and authenticate with the OpenSearch client\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWS4Auth(credentials.access_key, credentials.secret_key, REGION, 'aoss', session_token=credentials.token)\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': HOST, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=300\n",
    ")\n",
    "\n",
    "# Define number of images to search and retrieve\n",
    "K_SEARCHES = 3\n",
    "\n",
    "# Define search configuration body for K-NN \n",
    "body = {\n",
    "        \"size\": K_SEARCHES,\n",
    "        \"_source\": {\n",
    "            \"exclude\": [VECTOR_NAME],\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"vectors\": {\n",
    "                    \"vector\": object_embedding,\n",
    "                    \"k\": K_SEARCHES,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": True,\n",
    "        \"fields\": [VECTOR_MAPPING],\n",
    "    }\n",
    "\n",
    "# Invoke OpenSearch to search through index with K-NN configurations\n",
    "knn_response = client.search(index=INDEX_NAME, body=body)\n",
    "result = []\n",
    "scores_tracked = set()  # Set to keep track of already retrieved images and their scores\n",
    "\n",
    "# Loop through response to print the closest matching results\n",
    "for hit in knn_response[\"hits\"][\"hits\"]:\n",
    "    id_ = hit[\"_id\"]\n",
    "    score = hit[\"_score\"]\n",
    "    item_id_ = hit[\"_source\"][VECTOR_MAPPING]\n",
    "\n",
    "    # Check if score has already been tracked, if not, add it to final result\n",
    "    if score not in scores_tracked:\n",
    "        final_item = [item_id_, score]\n",
    "        result.append(final_item)\n",
    "        scores_tracked.add(score)  # Log score as tracked already\n",
    "\n",
    "# Print Top K closest matches\n",
    "print(f\"Top {K_SEARCHES} closest embeddings and associated scores: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713c369-592f-4c2b-af0e-89a57c7c47de",
   "metadata": {},
   "source": [
    "### 6.2 Display similar images from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a747e-0fcc-43f0-8dd0-2364a300a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to display image\n",
    "def display_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image.show()\n",
    "\n",
    "# List of image file names from the K-NN search\n",
    "image_files = result\n",
    "\n",
    "# Create a local directory to store downloaded images\n",
    "download_dir = 'RESULTS'\n",
    "\n",
    "# Create directory if not exists\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download and display each image that matches image query\n",
    "for file_name in image_files:\n",
    "    print(\"File Name: \" + file_name[0])\n",
    "    print(\"Score: \" + str(file_name[1]))\n",
    "    local_path = os.path.join(download_dir, file_name[0])\n",
    "    # Ensure to add in necessary prefix before file name if files are in subdirectories in the bucket\n",
    "    # ex. s3.download_file(BUCKET_NAME, \"Shoes/Sneakers and Athletic Shoes/Nike/"\+file_name[0], local_path)\n",
    "    s3.download_file(BUCKET_NAME, file_name[0], local_path)\n",
    "    # Open downloaded image and display it\n",
    "    display_image(local_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f0b83-3e63-494d-b8b6-54fd0d6e8e96",
   "metadata": {},
   "source": [
    "## 7. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14efc1a-fdfc-48fe-bd62-f51b60258e9e",
   "metadata": {},
   "source": [
    "To remove resources after usage, perform the following steps in your AWS Management console:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad73c23-9864-47f2-bb28-48c130c6b523",
   "metadata": {},
   "source": [
    "Empty and delete the S3 bucket. Delete the index in the OpenSearch Serverless collection. Delete the OpenSearch Serverless collection. Delete SageMaker Studio user profile and domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
